{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev.showdoc as literacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from speechsep.imports import *\n",
    "from speechsep.utils import *\n",
    "from speechsep.plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains most of the basic functions and spectrogram class types. To visualize the spectrograms we will also include a special color map since this makes it easier to notice differences in audio intensities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important things to remember are\n",
    "- How to create an AudioItem from file, both mono and multi-channel.\n",
    "- Creating a SpecImage and how the parameters influence the final result.\n",
    "- Basic SpecImage Visualizer (more indepth explanation here***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(load)\n",
    "def load_audio(fn, **kwargs):\n",
    "    return load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fn = Path(\"../data/AudioTest1.wav\")\n",
    "sig, sr = load_audio(fn)\n",
    "display(Audio(sig, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(sig), np.ndarray)\n",
    "test_eq(type(sr), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## AudioBase\n",
    "The current base class for audio which is used for mono and multi-channel audio types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def ResampleSignal(sr_new):\n",
    "    def _inner(sig, sr):\n",
    "        '''Resample using faster polyphase technique and avoiding FFT computation. Taken from FastaiAudio by LimeAI'''\n",
    "        if(sr == sr_new): return sig\n",
    "        sr_gcd = math.gcd(sr, sr_new)\n",
    "        resampled = resample_poly(sig, int(sr_new/sr_gcd), int(sr/sr_gcd), axis=-1)\n",
    "        #resampled = resampled.astype(np.float32)\n",
    "        return resampled\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioBase():\n",
    "    _show_args={}\n",
    "    def __init__(self,sig,_sr,fn=None):\n",
    "        store_attr(self, 'sig,_sr,fn')\n",
    "        self.data = self.sig\n",
    "    def __repr__(self): self.listen(); return f'{self.__str__()}'\n",
    "    def __str__(self): return f'{self.fn}, {self.duration}secs at {self.sr} samples per second'\n",
    "    def listen(self): display(Audio(self.sig, rate=self.sr))\n",
    "    @property\n",
    "    def sr(self): return self._sr\n",
    "    @sr.setter\n",
    "    def sr(self, new_sr):\n",
    "        if self._sr != new_sr: self.sig = ResampleSignal(new_sr)(self.sig, self.sr)\n",
    "        self._sr = new_sr\n",
    "    @property\n",
    "    def duration(self): return len(self.sig)/self.sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MonoAudios\n",
    "Audios with only one channel. For now this is the only Audio type, if the file has more channels they will be averaged out into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioMono(AudioBase):\n",
    "    _show_args={}\n",
    "    @classmethod\n",
    "    def create(cls, fn, sr=None):\n",
    "        audio = cls(*load_audio(fn),fn)\n",
    "        if sr: audio.sr = sr\n",
    "        return audio\n",
    "    load_file = create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aud1 = AudioMono.create(fn) #default file sample rate\n",
    "aud2 = AudioMono.create(fn, sr=2205) #custom sample rate, could cause loss of quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(aud1), AudioMono)\n",
    "test_eq(aud1.sr, 22050)\n",
    "test_eq(aud1.fn, fn)\n",
    "\n",
    "test_eq(aud2.sr, 2205)\n",
    "test_eq(type(aud2.sig), np.ndarray)\n",
    "test_eq(type(aud2.sr), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_property\n",
    "def duration(x:AudioMono):\n",
    "    return len(x.sig)/x.sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(aud1.duration), float)\n",
    "test_eq(round(aud1.duration), 4)\n",
    "\n",
    "aud1.sr = 48000\n",
    "\n",
    "test_eq(aud1.sr, 48000)\n",
    "test_eq(round(aud1.duration), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpecImage\n",
    "Gives the template for the rest of the Spectrogram classes. There will be transforms to add mel-bin and decibels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpecImage():\n",
    "    _show_args={}\n",
    "    def __init__(self, data, sr, fn=None):\n",
    "        store_attr(self, 'data, sr, fn')\n",
    "        self._plt_params = {}\n",
    "    @property\n",
    "    def plt_params(self): return self._plt_params\n",
    "    @plt_params.setter\n",
    "    @delegates(plt.pcolormesh)\n",
    "    def plt_params(self, **kwargs):\n",
    "        self._plot = partial(plt.pcolormesh, **kwargs)\n",
    "        self._plt_params = dict(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decibel Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decibelify\n",
    "Turn spectrogram amplitude to decibel, is automatically called in `Spectify` with `decibel=True`. Decibel is the same as amplitude (intensity of each \"pixel\") in log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decibelify(Transform):\n",
    "    def __init__(self): pass\n",
    "    def encodes(self,spec): pass\n",
    "    def decodes(self,spec): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-bin Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-binify\n",
    "Transforms the frequency to mel-bin. Just like decibels, this transform also resembles human hearing better than linear frequencies do. Sadly making mel-bins also makes it dificult to reconstruct the audio since the phase and data loss is very high. Recommended for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mel_Binify(Transform):\n",
    "    def __init__(self): pass\n",
    "    def encodes(self,spec): pass\n",
    "    def decodes(self,spec): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskBase():\n",
    "    def __init__(self, data):\n",
    "        store_attr(self, 'data')\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    @classmethod\n",
    "    def create(cls, audios):\n",
    "        self.adjust(audios)\n",
    "        joined = join_audios(audios)\n",
    "        return [cls(self.generate(joined, aud)) for aud in audios]\n",
    "    def adjust(self, audios):\n",
    "        pass\n",
    "    def __mult__(self, spec):\n",
    "        raise NotImplementedError('This function needs to be implemented before use')\n",
    "    def generate(self, joined, aud):\n",
    "        raise NotImplementedError('This function needs to be implemented before use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskBinary(MaskBase):\n",
    "    def __mult__(self, spec): pass\n",
    "    def __generate__(self, joined, aud): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:(AudioBase, SpecImage, MaskBase), y, samples, ctxs=None, max_n=10, rows=None, cols=None, figsize=None, **kwargs):\n",
    "    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, figsize=figsize)\n",
    "    ctxs = show_batch[object](x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    return ctxs\n",
    "\n",
    "def pre_plot(o, cls, ax=None, pltsize=None, ctx=None):\n",
    "    ax = ifnone(ax,ctx)\n",
    "    if ax is None: _,ax = plt.subplots(figsize=pltsize)\n",
    "    if isinstance(o, cls): o = o.data;\n",
    "    elif not isinstance(o,np.ndarray): o=array(o)\n",
    "    return ax, o\n",
    "\n",
    "def post_plot(ax, title, x_label, y_label, axis=False):\n",
    "    if title is not None: ax.set_title(title)\n",
    "    if x_label is not None: ax.set_xlabel(x_label)\n",
    "    if y_label is not None: ax.set_ylabel(y_label)\n",
    "    if not axis: ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "@patch\n",
    "@delegates(Line2D)\n",
    "def show(x:AudioBase, ctx=None, **kwargs): return show_audio(x, ctx=ctx, **merge(x._show_args, kwargs))\n",
    "\n",
    "@delegates(plt.plot)\n",
    "def show_audio(aud, ax=None, pltsize=None, title=None, ctx=None, x_label=None, y_label=None, axis=False, **kwargs):\n",
    "    ax, aud = pre_plot(aud, AudioBase, ax, pltsize, ctx)\n",
    "    ax.plot(aud, **kwargs)\n",
    "    return post_plot(ax, title, x_label, y_label, axis)\n",
    "\n",
    "@patch\n",
    "@delegates(setup_graph)\n",
    "def show(x:SpecImage, ctx=None, **kwargs): return show_spec(x, ctx=ctx, **merge(x._show_args, kwargs))\n",
    "\n",
    "@delegates(plt.pcolormesh)\n",
    "def show_spec(spec, ax=None, pltsize=None, title=None, ctx=None, x_label=None, y_label=None, axis=False, **kwargs):\n",
    "    ax, spec = pre_plot(spec, SpecImage, ax, pltsize, ctx)\n",
    "    ax.pcolormesh(np.abs(spec.data[:spec.data.shape[0]//2]), **kwargs)\n",
    "    return post_plot(ax, title, x_label, y_label, axis)\n",
    "\n",
    "@patch\n",
    "@delegates(setup_graph)\n",
    "def show(x:MaskBase, ctx=None, **kwargs): return show_mask(x, ctx=ctx, **merge(x._show_args, kwargs))\n",
    "\n",
    "@delegates(plt.pcolormesh)\n",
    "def show_mask(mask, ax=None, pltsize=None, title=None, ctx=None, x_label=None, y_label=None, axis=False, **kwargs):\n",
    "    ax, mask = pre_plot(mask)\n",
    "    ax.pcolormesh(maks, **kwargs)\n",
    "    return post_plot(ax, title, x_label, y_label, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hear_audio(aud, sr=48000, **kwargs):\n",
    "    if isinstance(aud, AudioBase):  display(Audio(aud.sig, rate=aud.sr))\n",
    "    else:                           display(Audio(aud, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ArrayAudioBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_audio(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "class ArraySpecBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_spec(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "class ArrayMaskBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_mask(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "class TensorAudio(TensorBase): \n",
    "    _show_args = ArrayAudioBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_audio(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "    \n",
    "class TensorSpec(TensorBase): \n",
    "    _show_args = ArraySpecBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_spec(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "    \n",
    "class TensorMask(TensorBase): \n",
    "    _show_args = ArrayMaskBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_mask(self, ctx=ctx, **{**self._show_args, **kwargs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "AudioMono._tensor_cls = TensorAudio\n",
    "SpecImage._tensor_cls = TensorSpec\n",
    "MaskBase._tensor_cls = TensorMask\n",
    "@ToTensor\n",
    "def encodes(self, o:AudioBase): return o._tensor_cls(audio2tensor(o))\n",
    "@ToTensor\n",
    "def encodes(self, o:SpecImage): return o._tensor_cls(spec2tensor(o))\n",
    "@ToTensor\n",
    "def encodes(self, o:MaskBase):  return o._tensor_cls(mask2tensor(o))\n",
    "\n",
    "def audio2tensor(aud:AudioBase): return TensorAudio(aud.sig)\n",
    "def spec2tensor(spec:SpecImage): return TensorSpec(spec.data)\n",
    "def mask2tensor(mask:MaskBase):  return TensorMask(mask.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(ToTensor()(aud1)), TensorAudio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectify\n",
    "Transform that turns AudioItem into a Spectrogram, it can take the parameters for decibel and mel_bin, which are the main transformations that are used. Standard problems will require decibels because it resembles human hearing. Mel-bins also achieve this but it requires us to loose large portion of the phase which reduces the intelligibility of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Spectify(Transform):\n",
    "    def __init__(self, sample_rate=48000, fftsize=512, win_mult=2, overlap=0.5, decibel=False, mel_bin=False):\n",
    "        store_attr(self, 'sample_rate, fftsize, win_mult, overlap, decibel, mel_bin')\n",
    "    def encodes(self, audio:AudioMono):\n",
    "        spec = stft(audio.sig, self.fftsize, self.win_mult, self.overlap)\n",
    "        if self.decibel: pass #TODO Encode\n",
    "        if self.mel_bin: pass #TODO Encode\n",
    "        return SpecImage(spec, audio.sr, audio.fn)\n",
    "    def decodes(self, spec:(ArraySpecBase, SpecImage)):\n",
    "        if self.mel_bin: pass #TODO Decode\n",
    "        if self.decibel: pass #TODO Decode\n",
    "        print(f\"in decode {type(spec)}\")\n",
    "        if isinstance(spec, SpecImage):\n",
    "            audio = istft(spec.data, self.fftsize, self.win_mult, self.overlap)\n",
    "            return AudioMono(audio, spec.sr, spec.fn)\n",
    "        audio = istft(spec, self.fftsize, self.win_mult, self.overlap)\n",
    "        return AudioMono(audio, self.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioMono.load_file(fn)\n",
    "Audio2Spec = Spectify()\n",
    "spec = Audio2Spec(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(spec), SpecImage)\n",
    "test_eq(type(spec.data), np.ndarray)\n",
    "test_eq(spec.fn, fn)\n",
    "test_eq(spec.sr, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_r = Audio2Spec.decodes(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(audio_r), AudioMono)\n",
    "test_eq(type(audio_r.sig), np.ndarray)\n",
    "test_eq(audio_r.sr, 22050)\n",
    "test_eq(audio_r.fn, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_clsmthd\n",
    "@delegates(to=Spectify)\n",
    "def create(cls:SpecImage, fn, sr=None, **kwargs):\n",
    "    #Open an `Audio` from path `fn`\n",
    "    if isinstance(fn,(Path,str)): return cls.create(AudioMono.create(fn,sr))\n",
    "    elif isinstance(fn,AudioMono): return Spectify(**kwargs)(fn)\n",
    "    raise ValueError('fn must be AudioMono, Path or str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = SpecImage.create(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(spec), SpecImage)\n",
    "test_eq(type(spec.data), np.ndarray)\n",
    "test_eq(spec.sr, 22050)\n",
    "test_eq(spec.fn, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasicTransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Resample(Transform):\n",
    "    def __init__(self, sr): self.sr = sr\n",
    "    def encodes(self, x:AudioBase): x.sr = self.sr; return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_resamp = Resample(sr//2)(AudioMono.create(fn))\n",
    "\n",
    "test_eq(audio_resamp.sr, 11025)\n",
    "test_eq(audio_resamp._sr, 11025)\n",
    "audio_resamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Clip(Transform):\n",
    "    def __init__(self, time): self.time = time\n",
    "    def encodes(self, x:AudioBase):\n",
    "        new_sig_len = int(self.time*x.sr)\n",
    "        diff = abs(len(x.sig) - new_sig_len)\n",
    "        if len(x.sig) <= new_sig_len:\n",
    "            x.sig = np.pad(x.sig, (0,diff), 'constant', constant_values=(0, 0))\n",
    "        else:\n",
    "            x.sig = x.sig[:new_sig_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ext = Clip(5)(AudioMono.create(fn))\n",
    "\n",
    "fn_long = Path(\"../data/AudioTest1_full.wav\")\n",
    "audio_clip = Clip(4)(AudioMono.create(fn_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(audio_ext.duration, 5.0)\n",
    "test_eq(len(audio_ext.sig), 5.0*audio_ext.sr)\n",
    "test_eq(audio_clip.duration, 4.0)\n",
    "test_eq(len(audio_clip.sig), 4.0*audio_clip.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase and Complex Tensor Managing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PhaseManager(Transform):\n",
    "    def __init__(self, mthd=\"new_dim\", cls=SpecImage):\n",
    "        assert mthd in ['new_dim', 'remove', 'replace'], 'phase method must be either new_dim, remove or replace'\n",
    "        store_attr(self, 'mthd, cls')\n",
    "        \n",
    "    def encodes(self, spec:SpecImage):\n",
    "        if self.mthd == 'new_dim': return complex2real(spec)\n",
    "        \n",
    "    def decodes(self, spec:TensorSpec)->SpecImage:\n",
    "        if self.mthd == 'new_dim':\n",
    "            return SpecImage(complex2real_r(spec),48000)\n",
    "\n",
    "        \n",
    "def complex2real(spec):\n",
    "    if np.iscomplexobj(spec.data):\n",
    "        spec.data = np.concatenate((spec.data.real[..., np.newaxis], spec.data.imag[..., np.newaxis]), axis=-1)\n",
    "        spec.data = spec.data.T\n",
    "    return spec\n",
    "\n",
    "def complex2real_r(data):\n",
    "    data = data.numpy().T\n",
    "    return data[..., 0] + data[..., 1]*1j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
