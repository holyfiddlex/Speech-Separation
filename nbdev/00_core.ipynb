{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev.showdoc as literacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from speechsep.imports import *\n",
    "from speechsep.utils import *\n",
    "from speechsep.plot import *\n",
    "from speechsep.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains most of the basic functions and spectrogram class types. To visualize the spectrograms we will also include a special color map since this makes it easier to notice differences in audio intensities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important things to remember are\n",
    "- How to create an AudioItem from file, both mono and multi-channel.\n",
    "- Creating a SpecImage and how the parameters influence the final result.\n",
    "- Basic SpecImage Visualizer (more indepth explanation here***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "load_audio = load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = Path(\"../data/AudioTest1.wav\")\n",
    "sig, sr = load_audio(fn)\n",
    "display(Audio(sig, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(sig), np.ndarray)\n",
    "test_eq(type(sr), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonoAudios\n",
    "Audios with only one channel. For now this is the only Audio type, if the file has more channels they will be averaged out into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioMono(AudioBase):\n",
    "    _show_args={}\n",
    "    @classmethod\n",
    "    def create(cls, fn, sr=None):\n",
    "        audio = cls(*load_audio(fn),fn)\n",
    "        if sr: audio.sr = sr\n",
    "        return audio\n",
    "    load_file = create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud1 = AudioMono.create(fn) #default file sample rate\n",
    "aud2 = AudioMono.create(fn, sr=2205) #custom sample rate, could cause loss of quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(aud1), AudioMono)\n",
    "test_eq(aud1.sr, 22050)\n",
    "test_eq(aud1.fn, fn)\n",
    "\n",
    "test_eq(aud2.sr, 2205)\n",
    "test_eq(type(aud2.sig), np.ndarray)\n",
    "test_eq(type(aud2.sr), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_property\n",
    "def duration(x:AudioMono):\n",
    "    return len(x.sig)/x.sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(aud1.duration), float)\n",
    "test_eq(round(aud1.duration), 4)\n",
    "\n",
    "aud1.sr = 48000\n",
    "\n",
    "test_eq(aud1.sr, 48000)\n",
    "test_eq(round(aud1.duration), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpecImage\n",
    "Gives the template for the rest of the Spectrogram classes. There will be transforms to add mel-bin and decibels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpecImage(SpecBase): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskBinary(MaskBase):\n",
    "    def __mult__(self, spec): pass\n",
    "    def __generate__(self, joined, aud): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:(AudioBase, SpecBase, MaskBase), y, samples, ctxs=None, max_n=10, rows=None, cols=None, figsize=None, **kwargs):\n",
    "    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, figsize=figsize)\n",
    "    ctxs = show_batch[object](x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    return ctxs\n",
    "\n",
    "@patch\n",
    "@delegates(Line2D)\n",
    "def show(x:AudioBase, ctx=None, **kwargs): return show_audio(x, ctx=ctx, **merge(x._show_args, kwargs))\n",
    "\n",
    "@patch\n",
    "@delegates(setup_graph)\n",
    "def show(x:SpecImage, ctx=None, **kwargs): return show_spec(x, ctx=ctx, **merge(x._show_args, kwargs))\n",
    "\n",
    "@patch\n",
    "@delegates(setup_graph)\n",
    "def show(x:MaskBase, ctx=None, **kwargs): return show_mask(x, ctx=ctx, **merge(x._show_args, kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ArrayAudioBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_audio(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "class ArraySpecBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_spec(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "class ArrayMaskBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_mask(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "\n",
    "class TensorAudio(TensorBase): \n",
    "    _show_args = ArrayAudioBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_audio(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "    \n",
    "class TensorSpec(TensorBase): \n",
    "    _show_args = ArraySpecBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_spec(self, ctx=ctx, **{**self._show_args, **kwargs})\n",
    "    \n",
    "class TensorMask(TensorBase): \n",
    "    _show_args = ArrayMaskBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_mask(self, ctx=ctx, **{**self._show_args, **kwargs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "AudioMono._tensor_cls = TensorAudio\n",
    "SpecImage._tensor_cls = TensorSpec\n",
    "MaskBase._tensor_cls = TensorMask\n",
    "@ToTensor\n",
    "def encodes(self, o:AudioBase): return o._tensor_cls(audio2tensor(o))\n",
    "@ToTensor\n",
    "def encodes(self, o:SpecBase): return o._tensor_cls(spec2tensor(o))\n",
    "@ToTensor\n",
    "def encodes(self, o:MaskBase):  return o._tensor_cls(mask2tensor(o))\n",
    "\n",
    "def audio2tensor(aud:AudioBase): return TensorAudio(aud.sig)\n",
    "def spec2tensor(spec:SpecBase): return TensorSpec(spec.data)\n",
    "def mask2tensor(mask:MaskBase):  return TensorMask(mask.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(ToTensor()(aud1)), TensorAudio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectify\n",
    "Transform that turns AudioItem into a Spectrogram, it can take the parameters for decibel and mel_bin, which are the main transformations that are used. Standard problems will require decibels because it resembles human hearing. Mel-bins also achieve this but it requires us to loose large portion of the phase which reduces the intelligibility of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Spectify(Transform):\n",
    "    def __init__(self, sr=48000, stft=stft, istft=istft):\n",
    "        store_attr(self, 'sr, stft, istft')\n",
    "    def encodes(self, audio:AudioMono):\n",
    "        spec = self.stft(audio.sig)\n",
    "        return SpecImage(spec, audio.sr, audio.fn)\n",
    "    def decodes(self, spec:SpecBase):\n",
    "        audio = self.istft(spec.data)\n",
    "        return AudioMono(audio, spec.sr, spec.fn)\n",
    "    def decodes(self, data:ArraySpecBase):\n",
    "        return SpecImage(data, self.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioMono.load_file(fn)\n",
    "Audio2Spec = Spectify()\n",
    "spec = Audio2Spec(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(spec), SpecImage)\n",
    "test_eq(type(spec.data), np.ndarray)\n",
    "test_eq(spec.fn, fn)\n",
    "test_eq(spec.sr, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_r = Audio2Spec.decodes(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(audio_r), AudioMono)\n",
    "test_eq(type(audio_r.sig), np.ndarray)\n",
    "test_eq(audio_r.sr, 22050)\n",
    "test_eq(audio_r.fn, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decibelify\n",
    "Turn spectrogram amplitude to decibel, is automatically called in `Spectify`. Decibel is the same as amplitude (intensity of each \"pixel\") in log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Decibelify(Transform):\n",
    "    def __init__(self): pass\n",
    "    def encodes(self,spec:SpecImage):\n",
    "        spec.data = np.log(spec.data)\n",
    "        return spec\n",
    "    def decodes(self,spec:SpecImage):\n",
    "        spec.data = np.exp(spec.data)\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-binify\n",
    "Transforms the frequency to mel-bin. Just like decibels, this transform also resembles human hearing better than linear frequencies do. Sadly making mel-bins also makes it dificult to reconstruct the audio since the phase and data loss is very high. Recommended for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from librosa.feature import melspectrogram\n",
    "class Mel_Binify_lib(Transform):\n",
    "    @delegates(melspectrogram)\n",
    "    def __init__(self, **kwargs):\n",
    "        self.audio2mel = partial(melspectrogram, **kwargs)\n",
    "    def encodes(self,audio:AudioBase):\n",
    "        spec = self.audio2mel(audio.sig, audio.sr)\n",
    "        return SpecImage(spec, audio.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mel_Binify(Transform):\n",
    "    def __init__(self): pass #TODO Parameters f_max f_min | check more on librosas melbin\n",
    "    #TODO Add librosa melbin straight from audio?\n",
    "    def encodes(self,spec:SpecBase): pass\n",
    "    def decodes(self,spec:SpecBase): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCCify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from librosa.feature import mfcc\n",
    "class MFCCify(Transform):\n",
    "    @delegates(mfcc)\n",
    "    def __init__(self, **kwargs):\n",
    "        self.audio2mfcc = partial(mfcc, **kwargs)\n",
    "    def encodes(self,audio:AudioBase):\n",
    "        spec = self.audio2mfcc(audio.sig, audio.sr)\n",
    "        return SpecImage(spec, audio.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch_clsmthd\n",
    "@delegates(to=Spectify)\n",
    "def create(cls:SpecImage, fn, sr=None, **kwargs):\n",
    "    #Open an `Audio` from path `fn`\n",
    "    if isinstance(fn,(Path,str)): return cls.create(AudioMono.create(fn,sr))\n",
    "    elif isinstance(fn,AudioMono): return Spectify(**kwargs)(fn)\n",
    "    raise ValueError('fn must be AudioMono, Path or str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = SpecImage.create(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(spec), SpecImage)\n",
    "test_eq(type(spec.data), np.ndarray)\n",
    "test_eq(spec.sr, 22050)\n",
    "test_eq(spec.fn, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasicTransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Resample(Transform):\n",
    "    def __init__(self, sr): self.sr = sr\n",
    "    def encodes(self, x:AudioBase): x.sr = self.sr; return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_resamp = Resample(sr//2)(AudioMono.create(fn))\n",
    "\n",
    "test_eq(audio_resamp.sr, 11025)\n",
    "test_eq(audio_resamp._sr, 11025)\n",
    "audio_resamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Clip(Transform):\n",
    "    def __init__(self, time): self.time = time\n",
    "    def encodes(self, x:AudioBase):\n",
    "        new_sig_len = int(self.time*x.sr)\n",
    "        diff = abs(len(x.sig) - new_sig_len)\n",
    "        if len(x.sig) <= new_sig_len:\n",
    "            x.sig = np.pad(x.sig, (0,diff), 'constant', constant_values=(0, 0))\n",
    "        else:\n",
    "            x.sig = x.sig[:new_sig_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ext = Clip(5)(AudioMono.create(fn))\n",
    "\n",
    "fn_long = Path(\"../data/AudioTest1_full.wav\")\n",
    "audio_clip = Clip(4)(AudioMono.create(fn_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(audio_ext.duration, 5.0)\n",
    "test_eq(len(audio_ext.sig), 5.0*audio_ext.sr)\n",
    "test_eq(audio_clip.duration, 4.0)\n",
    "test_eq(len(audio_clip.sig), 4.0*audio_clip.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase and Complex Tensor Managing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PhaseManager(Transform):\n",
    "    def __init__(self, mthd=\"new_dim\", cls=SpecImage):\n",
    "        assert mthd in ['new_dim', 'remove', 'replace'], 'phase method must be either new_dim, remove or replace'\n",
    "        store_attr(self, 'mthd, cls')\n",
    "        \n",
    "    def encodes(self, spec:SpecImage):\n",
    "        if self.mthd == 'new_dim': return complex2real(spec)\n",
    "        \n",
    "    #BUG ArraySpecBase not Casting to return value\n",
    "    def decodes(self, spec:TensorSpec)->ArraySpecBase:\n",
    "        if self.mthd == 'new_dim':\n",
    "            spec = real2complex(spec)\n",
    "            #HACK not sure how else to get the output to be and ArraySpecBase\n",
    "            # If this is removed Spectify would have to decode a numpy array and that's not always what we want.\n",
    "            # If it doesn't find how to decode an ndarray it will try to show and ndarray doesn't have that function\n",
    "            temp = ArraySpecBase(spec.shape, dtype=np.complex)\n",
    "            temp[:,:] = spec\n",
    "            return temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
