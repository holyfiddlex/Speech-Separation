{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev.showdoc as literacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from speechsep.imports import *\n",
    "from speechsep.utils import stft,istft,ResampleSignal\n",
    "from speechsep.plot import setup_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains most of the basic functions and spectrogram class types. To visualize the spectrograms we will also include a special color map since this makes it easier to notice differences in audio intensities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important things to remember are\n",
    "- How to create an AudioItem from file, both mono and multi-channel.\n",
    "- Creating a SpecImage and how the parameters influence the final result.\n",
    "- Basic SpecImage Visualizer (more indepth explanation here***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(load)\n",
    "def load_audio(fn, **kwargs):\n",
    "    return load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = Path(\"../data/AudioTest1.wav\")\n",
    "sig, sr = load_audio(fn)\n",
    "display(Audio(sig, rate=sr))\n",
    "\n",
    "test_eq(type(sig), np.ndarray)\n",
    "test_eq(type(sr), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioBase\n",
    "The current base class for audio which is used for mono and multi-channel audio types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioBase():\n",
    "    def __init__(self,sig,_sr,fn=None):\n",
    "        store_attr(self, 'sig,_sr,fn')\n",
    "    def __repr__(self): display(Audio(self.sig, rate=self.sr)); return f'{self.__str__()}'\n",
    "    def __str__(self): return f'{self.fn}, {len(self.sig)/self.sr}secs at {self.sr} samples per second'\n",
    "    @delegates(Line2D)\n",
    "    def show(self, **kwargs): plt.plot(self.sig, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonoAudios\n",
    "Audios with only one channel. For now this is the only Audio type, if the file has more channels they will be averaged out into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioMono(AudioBase):\n",
    "    @classmethod\n",
    "    def create(cls, fn, sr=None):\n",
    "        audio = cls(*load_audio(fn),fn)\n",
    "        if sr: audio.sr = sr\n",
    "        return audio\n",
    "    load_file = create\n",
    "    @property\n",
    "    def sr(self): return self._sr\n",
    "    @sr.setter\n",
    "    def sr(self, new_sr):\n",
    "        if self._sr != new_sr: self.sig = ResampleSignal(new_sr)(self.sig, self.sr)\n",
    "        self._sr = new_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud1 = AudioMono.create(fn) #default file sample rate\n",
    "aud2 = AudioMono.create(fn, sr=2205) #custom sample rate, could cause loss of quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(aud1), AudioMono)\n",
    "test_eq(aud1.sr, 22050)\n",
    "test_eq(aud1.fn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(aud2.sr, 2205)\n",
    "test_eq(type(aud2.sig), np.ndarray)\n",
    "test_eq(type(aud2.sr), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_property\n",
    "def duration(x:AudioMono):\n",
    "    return len(x.sig)/x.sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(aud1.duration), float)\n",
    "test_eq(round(aud1.duration), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud1.sr = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(aud1.sr, 48000)\n",
    "test_eq(round(aud1.duration), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensor and Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayAudioBase(ArrayBase):\n",
    "    _show_args = {}\n",
    "    def show(self, **kwargs):\n",
    "        return show_audio(self, ctx=ctx, **{**self._show_args, **kwargs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorAudio(TensorBase): \n",
    "    _show_args = ArrayAudioBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2tensor(aud:AudioBase): return Tensor(aud.sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioMono._tensor_cls = TensorAudio\n",
    "@ToTensor\n",
    "def encodes(self, o:AudioBase): return o._tensor_cls(audio2tensor(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typedispatch\n",
    "def show_batch(x:AudioMono, y, samples, ctxs=None, max_n=10, rows=None, cols=None, figsize=None, **kwargs):\n",
    "    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, figsize=figsize)\n",
    "    ctxs = show_batch[object](x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorAudio(Tensor(aud1.sig))\n",
    "\n",
    "test_eq(type(ToTensor()(aud1)), TensorAudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_audio(aud):\n",
    "    display(Audio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpecImage\n",
    "Gives the template for the rest of the Spectrogram classes. There will be transforms to add mel-bin and decibels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpecImage():\n",
    "    def __init__(self, data, sr, fn=None):\n",
    "        store_attr(self, 'data, sr, fn')\n",
    "        self._plt_params = {}\n",
    "    @property\n",
    "    def plt_params(self): return self._plt_params\n",
    "    @plt_params.setter\n",
    "    @delegates(plt.pcolormesh)\n",
    "    def plt_params(self, **kwargs):\n",
    "        self._plot = partial(plt.pcolormesh, **kwargs)\n",
    "        self._plt_params = dict(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectify\n",
    "Transform that turns AudioItem into a Spectrogram, it can take the parameters for decibel and mel_bin, which are the main transformations that are used. Standard problems will require decibels because it resembles human hearing. Mel-bins also achieve this but it requires us to loose large portion of the phase which reduces the intelligibility of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Spectify(Transform):\n",
    "    def __init__(self, fftsize=512, win_mult=2, overlap=0.5, decibel=False, mel_bin=False):\n",
    "        store_attr(self, 'fftsize, win_mult, overlap, decibel, mel_bin')\n",
    "    def encodes(self, audio:AudioMono):\n",
    "        spec = stft(audio.sig, self.fftsize, self.win_mult, self.overlap)\n",
    "        if self.decibel: pass #TODO Encode\n",
    "        if self.mel_bin: pass #TODO Encode\n",
    "        return SpecImage(spec, audio.sr, audio.fn)\n",
    "    def decodes(self, spec):\n",
    "        audio = istft(spec.data, self.fftsize, self.win_mult, self.overlap)\n",
    "        if self.decibel: pass #TODO Decode\n",
    "        if self.mel_bin: pass #TODO Decode\n",
    "        return AudioMono(audio, spec.sr, spec.fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decibelify\n",
    "Turn spectrogram amplitude to decibel, is automatically called in `Spectify` with `decibel=True`. Decibel is the same as amplitude (intensity of each \"pixel\") in log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decibelify(Transform):\n",
    "    def __init__(self): pass\n",
    "    def encodes(self,spec): pass\n",
    "    def decodes(self,spec): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-binify\n",
    "Transforms the frequency to mel-bin. Just like decibels, this transform also resembles human hearing better than linear frequencies do. Sadly making mel-bins also makes it dificult to reconstruct the audio since the phase and data loss is very high. Recommended for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mel_Binify(Transform):\n",
    "    def __init__(self): pass\n",
    "    def encodes(self,spec): pass\n",
    "    def decodes(self,spec): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioMono.load_file(fn)\n",
    "Audio2Spec = Spectify()\n",
    "spec = Audio2Spec(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(spec), SpecImage)\n",
    "test_eq(type(spec.data), np.ndarray)\n",
    "test_eq(spec.fn, fn)\n",
    "test_eq(spec.sr, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "@delegates(setup_graph)\n",
    "def show(x:SpecImage, ctx=None, **kwargs):\n",
    "    setup_graph(**kwargs)\n",
    "    plt.pcolormesh(abs(x.data[:x.data.shape[0]//2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.show(title='one two', x_label='time', y_label='frequency', fig_size = [12,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_r = Audio2Spec.decodes(spec)\n",
    "audio_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(type(audio_r), AudioMono)\n",
    "test_eq(type(audio_r.sig), np.ndarray)\n",
    "test_eq(audio_r.sr, 22050)\n",
    "test_eq(audio_r.fn, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_clsmthd\n",
    "@delegates(to=Spectify)\n",
    "def create(cls:SpecImage, fn, sr=None, **kwargs):\n",
    "    #Open an `Audio` from path `fn`\n",
    "    if isinstance(fn,(Path,str)): return cls.create(AudioMono.create(fn,sr))\n",
    "    elif isinstance(fn,AudioMono): return Spectify(**kwargs)(fn)\n",
    "    raise ValueError('fn must be AudioMono, Path or str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = SpecImage.create(fn)\n",
    "spec.show(fig_size=[12,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskBase():\n",
    "    def __init__(self, data):\n",
    "        store_attr(self, 'data')\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    @classmethod\n",
    "    def create(cls, audios):\n",
    "        self.adjust(audios)\n",
    "        joined = join_audios(audios)\n",
    "        return [cls(self.generate(joined, aud)) for aud in audios]\n",
    "    def adjust(self, audios):\n",
    "        pass\n",
    "    def __mult__(self, spec):\n",
    "        raise NotImplementedError('This function needs to be implemented before use')\n",
    "    def generate(self, joined, aud):\n",
    "        raise NotImplementedError('This function needs to be implemented before use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskBinary(MaskBase):\n",
    "    def __mult__(self, spec): pass\n",
    "    def __generate__(self, joined, aud): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
