{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev.showdoc as literacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from speechsep.imports import *\n",
    "from speechsep.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains helper functions used for spectrogram creation and audio processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def time_bins(X, window_size, overlap):\n",
    "    \"\"\"\n",
    "    Create an overlapped version of X\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape=(n_samples,)\n",
    "        Input signal to window and overlap\n",
    "    window_size : int\n",
    "        Size of windows to take\n",
    "    window_step : int\n",
    "        Step size between windows\n",
    "    Returns\n",
    "    -------\n",
    "    X_strided : shape=(nun_windows, window_size)\n",
    "        2D array of overlapped X\n",
    "    \"\"\"\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(f\"Window size must be even! Recieved {window_size}\")\n",
    "    padding = np.zeros(int(window_size - len(X) % window_size))\n",
    "    X = np.concatenate((X, padding))\n",
    "    slide_length = int(window_size*(1-overlap))\n",
    "    num_windows = (len(X) - window_size) // slide_length\n",
    "    out = np.ndarray((num_windows,window_size),dtype = X.dtype)\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        start = i * slide_length\n",
    "        out[i] = X[start : start+window_size]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stft(X, fftsize=512, win_mult=2, overlap=0.5, normalize=False):\n",
    "    \"\"\"\n",
    "    Compute STFT for 1D real valued input X\n",
    "    \"\"\"\n",
    "    win_size = fftsize*win_mult\n",
    "    X = time_bins(X, win_size, overlap)\n",
    "    hanning = .54 - .46 * np.cos(2 * np.pi * np.arange(win_size) / (win_size - 1))\n",
    "    X = X * hanning.reshape((1, win_size))\n",
    "    X = np.fft.fft(X).T\n",
    "    #X = np.fft.fft(X)[:win_size//2].T\n",
    "    if normalize: X*=256/X.max()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def istft(X, fftsize=512, win_mult=2, overlap=0.5, normalize=False):\n",
    "    #X = np.concatenate((X, X[::-1]), axis=0)\n",
    "    X = np.fft.ifft(X.T).real\n",
    "    win_size = len(X[0])\n",
    "    slider_length = int(win_size*(1-overlap))\n",
    "\n",
    "    hanning = .54 - .46 * np.cos(2 * np.pi * np.arange(win_size) / (win_size - 1))\n",
    "    X = X/hanning.reshape((1, win_size))\n",
    "\n",
    "    inv_audio = X[0][0:win_size-slider_length]\n",
    "    for i in range(len(X)):\n",
    "        inv_audio = np.concatenate((inv_audio, X[i][-slider_length:]))\n",
    "    return(inv_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fill(sig, shape):\n",
    "    diff = abs(len(sig) - shape)\n",
    "    return np.pad(sig, (0,diff), 'constant', constant_values=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def randomComplex(shape):\n",
    "    randcmplx = np.random.multivariate_normal([0,0], [[1,0],[0,1]], shape)\n",
    "    return randcmplx[:,:,0]+randcmplx[:,:,1]*1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def complex2real(data):\n",
    "    if np.iscomplexobj(data):\n",
    "        new_data = np.concatenate((data.real[..., np.newaxis], data.imag[..., np.newaxis]), axis=-1)\n",
    "        return new_data.T\n",
    "    return data\n",
    "\n",
    "def real2complex(data):\n",
    "    data = data.numpy().swapaxes(-1,-3)\n",
    "    return data[..., 0] + data[..., 1]*1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def complex_mult(x,y):\n",
    "    x_real, x_imag = x[...,:1,:,:], x[...,1:,:,:]\n",
    "    y_real, y_imag = y[...,:1,:,:], y[...,1:,:,:]\n",
    "    res_real = x_real*y_real - x_imag*y_imag\n",
    "    res_imag = x_real*y_imag + x_imag*y_real\n",
    "    return torch.cat((res_real,res_imag), -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_shape(olist):\n",
    "    \"Get the shape of items in iterable. If there are different lengths an error will occur\"\n",
    "    shapes = set([o.shape for o in olist])\n",
    "    if len(shapes) > 1: ValueError(\"To generate Masks make sure that the length of files are equal.\")\n",
    "    return shapes.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_audios(audioList):\n",
    "    np_list = audioList if isinstance(audioList[0], ndarray) else [audio.data for audio in audioList]\n",
    "    mix = np.zeros(get_shape(np_list))\n",
    "    for aud in np_list: mix+=aud\n",
    "    return mix\n",
    "\n",
    "class Mixer(Transform):\n",
    "    as_item_force=True\n",
    "    def encodes(self, audioList):\n",
    "        joined = join_audios(audioList)\n",
    "        AudioType, sr = type(audioList[0]), audioList[0].sr\n",
    "        return Tuple(AudioType(joined, sr)) + Tuple(audioList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Unet_Trimmer(Transform):\n",
    "    def __init__(self, trim_val):\n",
    "        self.trim_val=trim_val\n",
    "    def encodes(self, spec):\n",
    "        if isinstance(spec, SpecBase):\n",
    "            trim = spec.data.shape[1]//self.trim_val*self.trim_val\n",
    "            return type(spec)(spec.data[:,:trim], spec.sr, spec.fn)\n",
    "        trim = spec.shape[1]//self.trim_val*self.trim_val\n",
    "        return data[:,:trim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
