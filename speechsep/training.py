# AUTOGENERATED! DO NOT EDIT! File to edit: nbdev/04_Trainer.ipynb (unless otherwise specified).

__all__ = ['fn', 'pipe', 'Tensorify', 'AudioDataset', 'ext_MSELoss', 'loss_func', 'bs', 'shuffle', 'workers', 'dataset',
           'dataloader', 'dataiter', 'data', 'n_epochs', 'n_samples', 'n_iter', 'model']

# Cell
from .core import *
from .masks import *
from .pipe import *
from .utils import *
from .data import *
from .imports import *
from .models import *

# Cell
from torch.utils.data import Dataset, DataLoader
import torch
import torchvision

# Cell
fn = Path("../data/esc50_sample/")
pipe = AudioPipe(fn)

# Cell
class Tensorify(Transform):
    def encodes(self, x):
        tnsr = complex2real(x.data) if hasattr(x, "data") else complex2real(x)
        return torch.FloatTensor(tnsr)

class AudioDataset(Dataset):
    @delegates(AudioPipe)
    def __init__(self, fn, **kwargs):
        self.fn = fn
        self.pipe = AudioPipe(fn, **kwargs)
        self.n_samples = len(get_audio_files(fn))

    def __getitem__(self, index):
        x,y = self.pipe(index)
        x,y = Tensorify()(x),Tensorify()(y)
        return x,y

    def __len__(self):
        return self.n_samples

# Cell
def ext_MSELoss():
    return sumnn.MSELoss(sep, yb)

def loss_func(sep, yb):
    return min(,nn.MSELoss(sep[:,:,-1], yb))

# Cell
bs = 1
shuffle=True
workers=2

# Cell
dataset = AudioDataset(fn)
dataloader = DataLoader(dataset=dataset, batch_size=bs, shuffle=shuffle, num_workers=workers)

dataiter = iter(dataloader)

# Cell
data = dataiter.next()
data;

# Cell
n_epochs = 1
n_samples = len(dataset)
n_iter = math.ceil(n_samples/bs)

# Cell
model = U_Net(img_ch=2, output_ch=4)
model.train();

# Cell
for epoch in range(n_epochs):
    for i, (xb, yb) in enumerate(dataloader):
        out = model(xb)
        mask1 = MaskcIRM(out[:,:2,:,:])
        mask2 = MaskcIRM(out[:,2:,:,:])
        sep = mask1*xb, mask2*xb
        loss = loss_func(sep, yb)
        if (i+1)%5==0:
            print(f'epoch {epoch}: step {(i+1)/n_iter}')
        #loss.backward()
        #optimizer.step()
        #optimizer.zero_grad()